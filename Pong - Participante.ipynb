{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéÆ Pong\n",
    "\n",
    "Ol√° e bem vindo ao workshop de **reinforcement learning** (RL) do grupo Turing! O prop√≥sito desse projeto √© ensinar aos interessados como implementar um algoritmo revolucion√°rio - Q-learning - para resolver um dos maiores cl√°ssicos da hist√≥ria dos video-games, pong.\n",
    "\n",
    "Come√ßaremos falando sobre o problema, ou seja, sobre o jogo Pong. Este que foi o primeiro jogo de video-game lucrativo da hist√≥ria, publicado em 1972, o jogo consta 48 anos de legado. Pong simula uma partida de t√™nis, existem duas \"raquetes\" e uma bola, e o objetivo de cada uma das raquetes √© n√£o somente evitar que a bola passe por ela, como tamb√©m fazer com que esta passe pela linha que a outra raquete protege, criando assim a premissa que sustenta o interesse pelo jogo. Queremos ent√£o desenvolver um algoritmo capaz de - sem nenhuma explica√ß√£o adicional - maximizar as suas recompensas, sendo as a√ß√µes, os estados e as recompensas, todas relativas ao jogo Pong. Teremos no final, portanto, um modelo treinado capaz de bom desempenho dentro do ambiente. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos uma biblioteca chamada \"numpy\" para auxiliar nas computa√ß√µes. Esta √© uma biblioteca do python capaz de manuzear diversas computa√ß√µes matem√°ticas com maestria e nos ser√° importante para o trabalho vindouro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando a biblioteca Numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, √© importante definir nossas _vari√°veis globais_, as quais s√£o respons√°veis por regir valores que podem ser ajustados para melhor desempenho do modelo, ou maior velocidade de treinamento. Algumas dessas vari√°veis apenas far√£o sentido mais a frente, mas tentaremos dar explica√ß√µes sucintas para cada uma delas.\n",
    "\n",
    "* \"EPSILON\" ser√° o a vari√°vel correspondete √† letra grega \"$\\epsilon$\" de mesmo nome, que √© a nota√ß√£o usual para a probabilidade de explora√ß√£o do nosso agente. Para exemplificar a posi√ß√£o do nosso agente, que iluminar√° a fun√ß√£o dessa vari√°vel, ser√° dado um exemplo do cotidiano humano: Digamos que sua m√£e lhe concede algumas moedas de um real para comprar lanches na escola. Sua escola disp√µe de quatro m√°quinas de alimentos, mas todas elas possuem diferentes probabilidades de lhe conceder chocolates de marcas estranhas das quais voc√™ nunca ouviu falar. Como voc√™ deve aumentar a sua satisfa√ß√£o (conseguir mais frequentemente os chocolates mais gostosos) com um n√∫mero finito de recursos (as moedas que voc√™ possui) sem nunca saber com certeza o que esperar do seu investimento. Esse problema √© famoso na √°rea e √© conhecido como _the multi-armed bandit problem_, ent√£o como resolv√™-lo? Usaremos o algoritmo $\\epsilon$-guloso, o qual nos instrui a inicialmente explorar bastante at√© existir certo conhecimento do que esperar de cada m√°quina de chocolate e, conforme √© adquirida experi√™ncia, certa confi√¢ncia de que determinada m√°quina nos fornece com maior frequencia os chocolates de melhor gosto √© criada, ent√£o a necessidade de explorar ir√° decrescer. A vari√°vel em quest√£o, portanto, representa a probabilidade inicial de tentarmos uma a√ß√£o aleat√≥ria.\n",
    "\n",
    "* \"EPSILON_MIN\", conforme nossa confian√ßa aumenta, ser√° desej√°vel aproveitar nosso conhecimento com maior frequencia do que explorar ainda mais o ambiente. Contudo, sempre far√° sentido explorar ao menos um pouco enquanto treinamos o algoritmo e, portanto, √© criado um limite inferior para a probabilida de explora√ß√£o e tal √© a fun√ß√£o dessa vari√°vel.\n",
    "\n",
    "* \"DECAIMENTO\", como explicado, √© desejado diminuir a explora√ß√£o conforme a experi√™ncia aumenta. Essa vari√°vel √© justamente a taxa de decaimento da nossa probabilidade de explora√ß√£o.\n",
    "\n",
    "* \"ALFA\", algoritmos de aprendizado de m√°quina costumam precisar de uma forma de serem otimizados. Q-learning trabalha em cima de gradientes, uma entidade matem√°tica que indica a dire√ß√£o para maximizar (ou minimizar) uma fun√ß√£o. Dispondo dessa dire√ß√£o, precisamos informar qual deve ser o tamanho do passo a ser dado antes de atualizar a nova \"dire√ß√£o ideal\".\n",
    "\n",
    "* \"GAMA\" √© a vari√°vel correspondente a letra grega de mesmo nome \"$\\gamma$\", a qual denota o quanto desejamos que nosso algoritmo considere eventos futuros. Se \"$\\gamma = 1$\", nosso algoritmo avaliar√° que a situa√ß√£o futura ser melhor que a atual √© t√£o importante quanto a recompensa da situa√ß√£o atual em si, por outro lado, se \"$\\gamma = 0$\", os eventos futuros n√£o apresentam import√¢ncia alguma para nosso algoritmo. \n",
    "\n",
    "* \"N_EPISODIOS\" dita quantas vezes o agente dever√° \"reviver\" o ambiente (vit√≥rias e derrotas) antes de acabar seu treinamento.\n",
    "\n",
    "* \"Q\" √© um dicion√°rio, ou seja, uma estrtura de dados capaz de buscar elementos de forma r√°pida. N√≥s o usaremos para guardar valores relativos √†s estimativas do algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constantes da Pol√≠tica Epsilon Greedy\n",
    "# Epsilon: probabilidade de experimentar uma a√ß√£o aleat√≥ria\n",
    "EPSILON = 0.7        # Valor inicial do epsilon\n",
    "EPSILON_MIN = 0.01   # Valor m√≠nimo de epsilon\n",
    "DECAIMENTO = 0.98    # Fator de deca√≠mento do epsilon (por epis√≥dio)\n",
    "\n",
    "# Hiperpar√¢metros do Q-Learning\n",
    "ALFA = 0.05          # Learning rate\n",
    "GAMA = 0.9           # Fator de desconto\n",
    "\n",
    "N_EPISODIOS = 250    # Quantidade de epis√≥dios que treinaremos\n",
    "\n",
    "# Dicion√°rio dos valores de Q\n",
    "# Chaves: estados; valores: qualidade Q atribuida a cada a√ß√£o\n",
    "Q = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.kdnuggets.com/images/reinforcement-learning-fig1-700.jpg)\n",
    "\n",
    "Antes de partir para o c√≥digo do algoritmo em si, alguns esclarecimentos do ponto de vista do agente sobre o ambiente devem ser feitos. Pong √© um jogo simples, consiste em controlar a sua raquete com esperan√ßa de maximizar a sua pontua√ß√£o e manter a do advers√°rio a menor poss√≠vel. Devemos, contudo, adicionar algum rigor √† essa express√£o, de forma que o algoritmo seja capaz de lidar com essa informa√ß√£o. \"controlar a raquete\" significa haver tr√™s poss√≠veis a√ß√µes: subir, descer ou n√£o se mover. Cada a√ß√£o tomada em cada estado deve nos retornar uma recompensa e deve nos levar a um novo estado. A nossa recompensa ser√° de +500 u.r. se pontuarmos, -500 u.r. se pontuarem sobre n√≥s e 0 caso contr√°rio. Finalmente, os estados ser√£o vetores dist√¢ncia entre a raquete a bola, caso jogado no modo f√°cil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importando a Biblioteca Gym\n",
    "import gym\n",
    "\n",
    "# Criando o nosso Ambiente: Pong\n",
    "env = gym.make(\"pong:turing-easy-v0\")\n",
    "\n",
    "# N√∫mero total de a√ß√µes: 3\n",
    "# 0 = parado; 1 = baixo; 2 = cima\n",
    "n_acoes = env.action_space.n\n",
    "\n",
    "print('N√∫mero de a√ß√µes:', n_acoes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "como foi dito, os estados que o nosso agente utilizar√° para tomar suas decis√µes s√£o vetores (\"setas\") que apontam da raquete controlada at√© a bola, imagine agora o caso onde esses vetores apresentassem apenas seus m√≥dulos como n√∫meros inteiros. Se a nossa tela possuisse 800 u.d. de largura e 600 u.d. de altura, o espa√ßo amostral dos diferentes vetores e, portanto, diferentes estados do ponto de vista do agente seria $2 \\times 800 \\times 600 = 960000$. Para um algoritmo que consiste em guardar estimativas do valor de cada a√ß√£o para cada estado, esse n√∫mero de estados exigiria n√£o somente guardar como atualizar cada um desses valores at√© chegar numa estimativa otimizada para os 3 milh√µes de valores. N√£o √© o ideal. Para simplificar (e agilizar) a situa√ß√£o, \"discretizar\" os nossos estados √© razo√°vel e esperado, faremos com que estados similares o suficiente sejam considerados como iguais e comparilhem das mesmas estimativas (n√£o faz sentido distinguir o vetor (502,234) do vetor (515,222))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretiza_estado(estado):\n",
    "    return tuple(round(x/10) for x in estado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Explicar tomada de a√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def escolhe_acao(env, Q, estado, epsilon):\n",
    "    # Se n√£o conhecermos ainda o estado, inicializamos o Q de cada a√ß√£o como 0\n",
    "    if estado not in Q.keys(): Q[estado] = [0] * n_acoes\n",
    "\n",
    "    # Escolhemos um n√∫mero aleat√≥rio com \"np.random.random()\"\n",
    "    # Se esse n√∫mero for menor que epsilon, tomamos uma a√ß√£o aleat√≥ria\n",
    "    if np.random.random() < epsilon:\n",
    "        # Escolhemos uma a√ß√£o aleat√≥ria, com env.action_space.sample()\n",
    "        acao = ...\n",
    "    else:\n",
    "        # Escolhemos a melhor a√ß√£o para o estado atual, com np.argmax()\n",
    "        acao = ...\n",
    "    return acao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Explicar fun√ß√£o de rodar partida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roda_partida(env, Q, renderiza=True):\n",
    "    # Resetamos o ambiente\n",
    "    estado = env.reset()\n",
    "\n",
    "    # Discretizamos o estado\n",
    "    estado = ...\n",
    "    \n",
    "    done = False\n",
    "    retorno = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Escolhemos uma a√ß√£o\n",
    "        acao = ...\n",
    "\n",
    "        # Tomamos nossa a√ß√£o escolhida e recebemos informa√ß√µes do pr√≥ximo estado\n",
    "        prox_estado, recompensa, done, info = ...\n",
    "\n",
    "        # Discretizamos o pr√≥ximo estado\n",
    "        prox_estado = ...\n",
    "\n",
    "        # Renderizamos o Ambiente\n",
    "        if renderiza:\n",
    "            env.render()\n",
    "\n",
    "        retorno += recompensa\n",
    "        estado = prox_estado\n",
    "\n",
    "    print(f'retorno {retorno:.1f},  '\n",
    "          f'placar {env.score[0]}x{env.score[1]}')\n",
    "    \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rodamos uma partida de Pon\n",
    "roda_partida(env, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèãÔ∏è‚Äç‚ôÄÔ∏è Treinamento\n",
    "\n",
    "TODO: Falar de Treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Explicar Bellman (o b√°sico, j√° foi abordado antes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atualiza_q(Q, estado, acao, recompensa, prox_estado):\n",
    "    # para cada estado ainda n√£o descoberto, iniciamos seu valor como nulo\n",
    "    if estado not in Q.keys(): Q[estado] = [0] * n_acoes\n",
    "    if prox_estado not in Q.keys(): Q[prox_estado] = [0] * n_acoes\n",
    "\n",
    "    # equa√ß√£o de Bellman\n",
    "    Q[estado][acao] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Explicar (brevemente?) pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def salva_tabela(Q, nome = 'model.pickle'):\n",
    "    with open(nome, 'wb') as pickle_out:\n",
    "        pickle.dump(Q, pickle_out)\n",
    "\n",
    "def carrega_tabela(nome = 'model.pickle'):\n",
    "    with open(nome, 'rb') as pickle_out:\n",
    "        return pickle.load(pickle_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Explicar fun√ß√£o de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treina(env, Q):\n",
    "    retornos = []      # retorno de cada epis√≥dio\n",
    "    epsilon = EPSILON\n",
    "\n",
    "    for episodio in range(1, N_EPISODIOS+1):\n",
    "        # resetar o ambiente e discretizar a a√ß√£o\n",
    "        ...\n",
    "        \n",
    "        done = False\n",
    "        retorno = 0\n",
    "        \n",
    "        while not done:\n",
    "            # escolher uma a√ß√£o\n",
    "            ...\n",
    "\n",
    "            # tomar a a√ß√£o\n",
    "            ...\n",
    "\n",
    "            # discretizar o pr√≥ximo estado\n",
    "            ...\n",
    "\n",
    "            atualiza_q(Q, estado, acao, recompensa, prox_estado)\n",
    "\n",
    "            retorno += recompensa\n",
    "            estado = prox_estado\n",
    "\n",
    "        # calcular o pr√≥ximo epsilon\n",
    "        epsilon = ...\n",
    "        epsilon = max(epsilon, EPSILON_MIN)\n",
    "\n",
    "        retornos.append(retorno)\n",
    "\n",
    "        if episodio % 10 == 0:\n",
    "            salva_tabela(Q)\n",
    "\n",
    "        print(f'epis√≥dio {episodio},  '\n",
    "              f'retorno {retorno:7.1f},  '\n",
    "              f'retorno m√©dio (√∫ltimos 10 epis√≥dios) {np.mean(retornos[-10:]):7.1f},  '\n",
    "              f'placar {env.score[0]}x{env.score[1]},  '\n",
    "              f'epsilon: {epsilon:.3f}')\n",
    "        \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "treina(env, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèì Testando nosso Agente Treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "roda_partida(env, Q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}